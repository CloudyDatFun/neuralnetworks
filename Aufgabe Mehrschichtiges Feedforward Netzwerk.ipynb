{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mehrschichtiges feedforward-Netz\n",
    "Nachfolgend sollen die Methoden `feedForward`, `backPropagation` vervollständigt werden.\n",
    "Um die Implementierungen zu prüfen, können die Methoden `checkFeedForward` und `checkBackPropagation` verwendet werden.\n",
    "Anschließend werden verschiedene Architekturen (Layer, Anzahl Neuronen, Aktivierungsfunktionen, Batch-Sizes, Learningrates) manuell ausgetestet werden, um für die Beispiel-Funktionen optimale Ergebnisse zu erzielen. Danach folgt eine Implementierung mit `torch.nn`, `torch.autograd` und `torch.optim` und ein weiterer Test der Architekturen. Abschließend werden alternatvie Optimierungsalgorithmen ausgetestet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Implementierung Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "%matplotlib inline\n",
    "# Mehrschichtiges Neuronalels Netzwerk\n",
    "# Eingaben:\n",
    "#   layers: Anzahl an Neuronen pro Layer\n",
    "#   activations: verwendete Aktivierungsfunktionen pro Layer\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.weights = [] # Shape in Layer-i [Neuronen in i, Neuronen in i+1]\n",
    "        self.biases = []  # Shape in Layer-i [1, Neuronen in i+1]\n",
    "        \n",
    "        # Initialisieren der Gewichte und Bias pro Layer mit Zufallswerten\n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(self.initializeWeight(layers[i], layers[i+1], self.activations[i]))\n",
    "            self.biases.append(self.initializeWeight(1, layers[i+1], self.activations[i]))\n",
    "    \n",
    "    def initializeWeight(self, outputDim, inputDim, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return torch.randn(outputDim, inputDim)\n",
    "        elif(name == 'linear'):\n",
    "            tensor = torch.empty(outputDim, inputDim)\n",
    "            torch.nn.init.kaiming_uniform_(tensor, a=math.sqrt(5))\n",
    "            return tensor\n",
    "        elif(name == 'relu'):\n",
    "            return torch.randn(outputDim, inputDim)*math.sqrt(2/inputDim)\n",
    "        elif(name == 'tanh'):\n",
    "            return torch.randn(outputDim, inputDim)*math.sqrt(1/inputDim)\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            tensor = torch.empty(outputDim, inputDim)\n",
    "            torch.nn.init.kaiming_uniform_(tensor, a=math.sqrt(5))\n",
    "            return tensor\n",
    "    \n",
    "    def getActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : torch.exp(x)/(1+torch.exp(x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = torch.clone(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        elif(name == 'tanh'):\n",
    "            return lambda x : torch.tanh(x)\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return lambda x: x\n",
    "\n",
    "    def getDerivitiveActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : torch.exp(x)/(1+torch.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = torch.clone(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        elif(name == 'tanh'):\n",
    "            return lambda x : 1 - torch.tanh(x)**2\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return lambda x: 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        (_, a_s) = self.feedForward(X)\n",
    "        return a_s[-1]\n",
    "    \n",
    "    # Berechnung der Ausgaben\n",
    "    # Es werden alle Summen vor und nach der Aktivierungsfunktion pro Layer zurückgegeben,\n",
    "    # damit sie für die Backpropagation benutzt werden künenn\n",
    "    # Eingaben:\n",
    "    #   x: Eingabe an den ersten Layer [batch_size, 1]\n",
    "    # Ausgaben:\n",
    "    #   z_s: Summen (vor Aktivierungsfunktion) pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   a_s: Ausgaben pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    def feedForward(self, x):\n",
    "        a = x.clone()\n",
    "        z_s = []\n",
    "        a_s = [a] \n",
    "                      \n",
    "        for l in range(len(self.weights)):\n",
    "            z_s.append(a.mm(self.weights[l]) + self.biases[l])\n",
    "            activationFunction = self.getActivationFunction(self.activations[l])\n",
    "            a_s.append(activationFunction(z_s[-1]))\n",
    "            a = a_s[-1]\n",
    "\n",
    "        return (z_s, a_s)\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Zurückverteilen der Deltas an die Gewichte und Biases\n",
    "    # Eingaben:\n",
    "    #   z_s: Summen (vor Aktivierungsfunktion) pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   a_s: Ausgaben pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    # Ausgaben:\n",
    "    #   dw: Ableitungen bezüglich der Gewichte pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   db: Ableitungen bezüglich der Biases pro Layer, [batch_size, 1] \n",
    "    def backPropagation(self, y, z_s, a_s):\n",
    "            dw = []  # dC/dW\n",
    "            db = []  # dC/dB\n",
    "            deltas = [None] * len(self.weights)  # delta = dC/dZ Fehler pro Layer\n",
    "            # Füllen des Fehlers im letzten Layer anhand der erwarteten Ausgaben des Netzwerks\n",
    "            deltas[-1] = (y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1])(z_s[-1]))\n",
    "                       \n",
    "            # Ausführen der Backpropagation\n",
    "            # Füllen des Fehlers im Layer anhand der bisherigen Deltas\n",
    "            \n",
    "            for k in reversed(range(len(deltas)-1)):\n",
    "                deltas[k] = deltas[k+1].mm(self.weights[k+1].T)*(self.getDerivitiveActivationFunction(self.activations[k])(z_s[k]))\n",
    "                \n",
    "            batch_size = y.shape[0]\n",
    "            # Bestimmen der Gewichtsänderung\n",
    "            db = [torch.ones(1, batch_size).mm(d/float(batch_size)) for d in deltas]\n",
    "            db2 = [d/float(batch_size) for d in deltas]\n",
    "            dw = [a_s[i].T.mm(d)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "\n",
    "            print(db)\n",
    "            print(db2)\n",
    "\n",
    "            # Rückgabe der Ableitungen bezüglich der Gewichte und Biases\n",
    "            return dw, db\n",
    "\n",
    "    # Aktualisieren der Gewichte und Bias anhand der Ausgabe in mehreren Batches und Epochen\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "            for e in range(epochs): \n",
    "                i=0\n",
    "                while(i<y.shape[0]):\n",
    "                    x_batch = x[i:i+batch_size]\n",
    "                    y_batch = y[i:i+batch_size]\n",
    "                    i = i+batch_size\n",
    "                    z_s, a_s = self.feedForward(x_batch)\n",
    "                    dw, db = self.backPropagation(y_batch, z_s, a_s)\n",
    "                    self.weights = [w+lr*dweight for w,dweight in zip(self.weights, dw)]\n",
    "                    self.biases = [w+lr*dbias for w,dbias in zip(self.biases, db)]\n",
    "                    print(\"loss = {}\".format(torch.norm(a_s[-1]-y_batch)))\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSampleNeuralNetwork():\n",
    "    nn = NeuralNetwork(layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid'])\n",
    "    \n",
    "    # Feste Gewichte und Bias setzen, damit das Ergebnis immer gleich ist.\n",
    "    nn.weights = [\n",
    "        torch.tensor([\n",
    "            [ 0.1397, -0.5724,  0.2194, -3.0984,  0.6770, -0.3220,  1.4311,  0.0931, -0.7618,  0.6987],\n",
    "            [ 0.7182,  2.8919, -1.9848, -0.2261, -1.1376, -0.0176, -0.2824, -1.6781, -1.1798, -1.5255]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [ 3.0417],\n",
    "            [-1.1143],\n",
    "            [-2.2069],\n",
    "            [-1.1303],\n",
    "            [ 0.5879],\n",
    "            [ 0.5013],\n",
    "            [ 1.1043],\n",
    "            [ 0.8942],\n",
    "            [-1.3571],\n",
    "            [ 0.7487]\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    nn.biases = [\n",
    "        torch.tensor([\n",
    "            [ 0.7933,  0.5149, -1.4123,  0.6115,  0.0221, -1.2227,  1.4272, -0.9937, -0.5665, -0.6665]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-1.6589]]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkFeedForward():\n",
    "    nn = createSampleNeuralNetwork()\n",
    "    \n",
    "    # Feste Eingabewerte für den Test\n",
    "    X = torch.tensor([\n",
    "        [ 0.1397,  0.7182],\n",
    "        [-0.5724,  2.8919]\n",
    "    ]).T\n",
    "    \n",
    "    # Eingaben mit den Gewichten verarbeiten\n",
    "    a_s = nn.predict(X)\n",
    "    \n",
    "    # Erwartete Ausgabe des letzten Layers\n",
    "    expected = torch.tensor([\n",
    "            [0.4855000078678131, 0.7422999739646912]\n",
    "    ]).T\n",
    "    \n",
    "    assert(torch.allclose(expected, a_s, rtol = 1e-04, atol = 1e-04))\n",
    "    \n",
    "checkFeedForward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[tensor([[ 0.0200, -0.0077, -0.0212, -0.0049,  0.0030, -0.0012, -0.0039,  0.0086,\n         -0.0124,  0.0067]]), tensor([[-0.0222]])]\n[tensor([[ 0.0200, -0.0077, -0.0212, -0.0049,  0.0030, -0.0012, -0.0039,  0.0086,\n         -0.0124,  0.0067]]), tensor([[-0.0222]])]\n"
    }
   ],
   "source": [
    "def checkBackPropagation():\n",
    "    nn = createSampleNeuralNetwork()\n",
    "    \n",
    "     # Feste Eingabewerte für den Test\n",
    "    X = torch.tensor([\n",
    "        [ 0.1397, -0.5724],\n",
    "        [ 0.7182,  2.8919]\n",
    "    ])\n",
    "\n",
    "    \n",
    "    Y = torch.tensor([\n",
    "        [0.8000],\n",
    "        [0.1000]\n",
    "    ])\n",
    "    \n",
    "    # Eingaben mit den Gewichten verarbeiten\n",
    "    z_s, a_s = nn.feedForward(X)\n",
    "    \n",
    "    # Deltas bestimmen\n",
    "    dw, db = nn.backPropagation(Y, z_s, a_s)\n",
    "    \n",
    "    dwExpected = [\n",
    "        torch.tensor([\n",
    "            [-0.0022, -0.0011, -0.0029,  0.0027, -0.0007, -0.0028, -0.0059,  0.0011, -0.0012,  0.0007],\n",
    "            [-0.0415,  0.0044,  0.0126,  0.0233, -0.0086, -0.0152, -0.0298, -0.0055, 0.0102, -0.0055]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-0.0349],\n",
    "            [-0.0525],\n",
    "            [ 0.0172],\n",
    "            [ 0.0169],\n",
    "            [ 0.0232],\n",
    "            [-0.0025],\n",
    "            [-0.0178],\n",
    "            [ 0.0193],\n",
    "            [ 0.0190],\n",
    "            [ 0.0220]\n",
    "        ])   \n",
    "    ]\n",
    "    \n",
    "    dbExpected = [\n",
    "        torch.tensor([\n",
    "            [ 0.0200, -0.0077, -0.0212, -0.0049,  0.0030, -0.0012, -0.0039,  0.0086, -0.0124,  0.0067]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-0.0222]\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    assert(torch.allclose(dw[0], dwExpected[0], rtol = 1e-04, atol = 1e-04))\n",
    "    assert(torch.allclose(dw[1], dwExpected[1], rtol = 1e-04, atol = 1e-04))\n",
    "    assert(torch.allclose(db[0], dbExpected[0], rtol = 1e-04, atol = 1e-04))    \n",
    "    assert(torch.allclose(db[1], dbExpected[1], rtol = 1e-04, atol = 1e-04))\n",
    "    \n",
    "checkBackPropagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Optimierung der Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "sinus_nn = NeuralNetwork(null)\n",
    "\n",
    "# sin\n",
    "sinus_X = 2*math.pi*torch.rand(2000).reshape(-1, 1)\n",
    "# Je nach Ausgangslayer, muss ggf. eine Transformation sattfinden\n",
    "# Der Sigmoid hat z.B einen Wertebereich von (0,1) und könnte somit,\n",
    "# negativen Werte des Sinus nicht darstellen.\n",
    "sinus_y = (torch.sin(sinus_X) + 1) / 2\n",
    "\n",
    "sinus_nn.train(sinus_X, sinus_y, epochs=null, batch_size=null, lr = null)\n",
    "sinus_a_s = sinus_nn.predict(sinus_X)\n",
    "#print(y, X)\n",
    "plt.scatter(sinus_X.flatten(), sinus_y.flatten())\n",
    "plt.scatter(sinus_X.flatten(), sinus_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetwork(null)\n",
    "\n",
    "nlinear_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = torch.clone(nlinear_X)\n",
    "nlinear_y[nlinear_y>=2] = 2\n",
    "nlinear_y[nlinear_y<1] = 1\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "#print(y, X)\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetwork(null)\n",
    "\n",
    "nlinear_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = torch.clone(nlinear_X)\n",
    "nlinear_y[nlinear_y>=1.5] = 2\n",
    "nlinear_y[nlinear_y<1.5] = 1\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetwork(null)\n",
    "\n",
    "nlinear_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = (10*nlinear_X**5 - 8*nlinear_X**4 - 6*nlinear_X**3 + 4*nlinear_X**2 - 2*nlinear_X + 2.3) / 2.5\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetwork(null)\n",
    "\n",
    "nlinear_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = (10*nlinear_X**5 - 8*nlinear_X**4 - 6*nlinear_X**3 + 4*nlinear_X**2 - 2*nlinear_X + 2.3) / 2.5\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Implementierung Backpropagation mit Autograd und Layern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "# Mehrschichtiges Neuronalels Netzwerk\n",
    "# Eingaben:\n",
    "#   layers: Anzahl an Neuronen pro Layer\n",
    "#   activations: verwendete Aktivierungsfunktionen pro Layer\n",
    "class NeuralNetworkTorch(nn.Module):\n",
    "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        super(NeuralNetworkTorch, self).__init__()\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.torchLayers = []\n",
    "        \n",
    "        # Initialisieren der Gewichte und Bias pro Layer mit Zufallswerten\n",
    "        for i in range(len(layers)-1):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            self.torchLayers.append(layer)\n",
    "            self.add_module('layer' + str(i), layer)\n",
    "    \n",
    "    def getActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return f.sigmoud\n",
    "        elif(name == 'linear'):\n",
    "            return f.linear\n",
    "        elif(name == 'relu'):\n",
    "            return F.relu\n",
    "        elif(name == 'tanh'):\n",
    "            return torch.tanh\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return f.linear\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.feedForward(X)[-1].detach()\n",
    "    \n",
    "    # Berechnung der Ausgaben\n",
    "    # Es werden alle Summen vor und nach der Aktivierungsfunktion pro Layer zurückgegeben,\n",
    "    # damit sie für die Backpropagation benutzt werden künenn\n",
    "    # Eingaben:\n",
    "    #   x: Eingabe an den ersten Layer\n",
    "    # Ausgaben:\n",
    "    #   a_s: Ausgaben pro Layer\n",
    "    def feedForward(self, x):\n",
    "        a = x.clone()\n",
    "        a_s = [a]\n",
    "        #TODO Feedforward\n",
    "        return a_s\n",
    "\n",
    "    # Aktualisieren der Gewichte und Bias anhand der Ausgabe in mehreren Batches und Epochen\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "        for e in range(epochs): \n",
    "            i=0\n",
    "            while(i<len(y)):\n",
    "                x_batch = Variable(x[i:i+batch_size])\n",
    "                y_batch = Variable(y[i:i+batch_size])\n",
    "                i = i+batch_size\n",
    "\n",
    "                #TODO Training mit Pytorch\n",
    "                print(\"loss = {}\".format(torch.norm(a_s[-1]-y_batch)))\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetworkTorch(null)\n",
    "\n",
    "nlinear_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = (10*nlinear_X**5 - 8*nlinear_X**4 - 6*nlinear_X**3 + 4*nlinear_X**2 - 2*nlinear_X + 2.3) / 2.5\n",
    "\n",
    "print(nlinear_X.shape)\n",
    "print(nlinear_y.shape)\n",
    "print(nlinear_nn)\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "#print(y, X)\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetworkTorch(null)\n",
    "\n",
    "nlinear_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = torch.clone(nlinear_X)\n",
    "nlinear_y[nlinear_y>=1.5] = 2\n",
    "nlinear_y[nlinear_y<1.5] = 1\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weitere optimale Netzwerke mit der PytorchImplementierung für die anderen Probleme ermitteln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Alternative Optimierungsalgorithmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weitere Optimierungsalgorithmen von Pytorch für die Probleme ausprobieren und bewerten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit49fff36c31ad458c9f75859cc9990ad8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}