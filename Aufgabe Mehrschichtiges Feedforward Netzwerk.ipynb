{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mehrschichtiges feedforward-Netz\n",
    "Nachfolgend sollen die Methoden `feedForward`, `backPropagation` vervollständigt werden.\n",
    "Um die Implementierungen zu prüfen, können die Methoden `checkFeedForward` und `checkBackPropagation` verwendet werden.\n",
    "Anschließend werden verschiedene Architekturen (Layer, Anzahl Neuronen, Aktivierungsfunktionen, Batch-Sizes, Learningrates) manuell ausgetestet werden, um für die Beispiel-Funktionen optimale Ergebnisse zu erzielen. Danach folgt eine Implementierung mit `torch.nn`, `torch.autograd` und `torch.optim` und ein weiterer Test der Architekturen. Abschließend werden alternatvie Optimierungsalgorithmen ausgetestet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Implementierung Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "%matplotlib inline\n",
    "# Mehrschichtiges Neuronalels Netzwerk\n",
    "# Eingaben:\n",
    "#   layers: Anzahl an Neuronen pro Layer\n",
    "#   activations: verwendete Aktivierungsfunktionen pro Layer\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.weights = [] # Shape in Layer-i [Neuronen in i, Neuronen in i+1]\n",
    "        self.biases = []  # Shape in Layer-i [1, Neuronen in i+1]\n",
    "        \n",
    "        # Initialisieren der Gewichte und Bias pro Layer mit Zufallswerten\n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(self.initializeWeight(layers[i], layers[i+1], self.activations[i]))\n",
    "            self.biases.append(self.initializeWeight(1, layers[i+1], self.activations[i]))\n",
    "    \n",
    "    def initializeWeight(self, outputDim, inputDim, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return torch.randn(outputDim, inputDim)\n",
    "        elif(name == 'linear'):\n",
    "            tensor = torch.empty(outputDim, inputDim)\n",
    "            torch.nn.init.kaiming_uniform_(tensor, a=math.sqrt(5))\n",
    "            return tensor\n",
    "        elif(name == 'relu'):\n",
    "            return torch.randn(outputDim, inputDim)*math.sqrt(2/inputDim)\n",
    "        elif(name == 'tanh'):\n",
    "            return torch.randn(outputDim, inputDim)*math.sqrt(1/inputDim)\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            tensor = torch.empty(outputDim, inputDim)\n",
    "            torch.nn.init.kaiming_uniform_(tensor, a=math.sqrt(5))\n",
    "            return tensor\n",
    "    \n",
    "    def getActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : torch.exp(x)/(1+torch.exp(x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = torch.clone(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        elif(name == 'tanh'):\n",
    "            return lambda x : torch.tanh(x)\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return lambda x: x\n",
    "\n",
    "    def getDerivitiveActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : torch.exp(x)/(1+torch.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = torch.clone(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        elif(name == 'tanh'):\n",
    "            return lambda x : 1 - torch.tanh(x)**2\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return lambda x: 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        (_, a_s) = self.feedForward(X)\n",
    "        return a_s[-1]\n",
    "    \n",
    "    # Berechnung der Ausgaben\n",
    "    # Es werden alle Summen vor und nach der Aktivierungsfunktion pro Layer zurückgegeben,\n",
    "    # damit sie für die Backpropagation benutzt werden künenn\n",
    "    # Eingaben:\n",
    "    #   x: Eingabe an den ersten Layer [batch_size, 1]\n",
    "    # Ausgaben:\n",
    "    #   z_s: Summen (vor Aktivierungsfunktion) pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   a_s: Ausgaben pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    def feedForward(self, x):\n",
    "        a = x.clone()\n",
    "        z_s = []\n",
    "        a_s = [a] \n",
    "                      \n",
    "        for l in range(len(self.weights)):\n",
    "            z_s.append(a.mm(self.weights[l]) + self.biases[l])\n",
    "            activationFunction = self.getActivationFunction(self.activations[l])\n",
    "            a_s.append(activationFunction(z_s[-1]))\n",
    "            a = a_s[-1]\n",
    "\n",
    "        return (z_s, a_s)\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Zurückverteilen der Deltas an die Gewichte und Biases\n",
    "    # Eingaben:\n",
    "    #   z_s: Summen (vor Aktivierungsfunktion) pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   a_s: Ausgaben pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    # Ausgaben:\n",
    "    #   dw: Ableitungen bezüglich der Gewichte pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   db: Ableitungen bezüglich der Biases pro Layer, [batch_size, 1] \n",
    "    def backPropagation(self, y, z_s, a_s):\n",
    "            dw = []  # dC/dW\n",
    "            db = []  # dC/dB\n",
    "            deltas = [None] * len(self.weights)  # delta = dC/dZ Fehler pro Layer\n",
    "            # Füllen des Fehlers im letzten Layer anhand der erwarteten Ausgaben des Netzwerks\n",
    "            deltas[-1] = (y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1])(z_s[-1]))\n",
    "                       \n",
    "            # Ausführen der Backpropagation\n",
    "            # Füllen des Fehlers im Layer anhand der bisherigen Deltas\n",
    "            \n",
    "            for k in reversed(range(len(deltas)-1)):\n",
    "                deltas[k] = deltas[k+1].mm(self.weights[k+1].T)*(self.getDerivitiveActivationFunction(self.activations[k])(z_s[k]))\n",
    "                \n",
    "            batch_size = y.shape[0]\n",
    "            # Bestimmen der Gewichtsänderung\n",
    "            db = [torch.ones(1, batch_size).mm(d/float(batch_size)) for d in deltas]\n",
    "            dw = [a_s[i].T.mm(d)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "\n",
    "            # Rückgabe der Ableitungen bezüglich der Gewichte und Biases\n",
    "            return dw, db\n",
    "\n",
    "    # Aktualisieren der Gewichte und Bias anhand der Ausgabe in mehreren Batches und Epochen\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "            for e in range(epochs): \n",
    "                i=0\n",
    "                while(i<len(y)):\n",
    "                    x_batch = x[i:i+batch_size]\n",
    "                    y_batch = y[i:i+batch_size]\n",
    "                    i = i+batch_size\n",
    "                    z_s, a_s = self.feedForward(x_batch)\n",
    "                    dw, db = self.backPropagation(y_batch, z_s, a_s)\n",
    "                    self.weights = [w+lr*dweight for w,dweight in zip(self.weights, dw)]\n",
    "                    self.biases = [w+lr*dbias for w,dbias in zip(self.biases, db)]\n",
    "                    loss = torch.norm(a_s[-1]-y_batch)\n",
    "            return loss\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSampleNeuralNetwork():\n",
    "    nn = NeuralNetworkTorch(layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid'])\n",
    "    \n",
    "    # Feste Gewichte und Bias setzen, damit das Ergebnis immer gleich ist.\n",
    "    nn.weights = [\n",
    "        torch.tensor([\n",
    "            [ 0.1397, -0.5724,  0.2194, -3.0984,  0.6770, -0.3220,  1.4311,  0.0931, -0.7618,  0.6987],\n",
    "            [ 0.7182,  2.8919, -1.9848, -0.2261, -1.1376, -0.0176, -0.2824, -1.6781, -1.1798, -1.5255]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [ 3.0417],\n",
    "            [-1.1143],\n",
    "            [-2.2069],\n",
    "            [-1.1303],\n",
    "            [ 0.5879],\n",
    "            [ 0.5013],\n",
    "            [ 1.1043],\n",
    "            [ 0.8942],\n",
    "            [-1.3571],\n",
    "            [ 0.7487]\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    nn.biases = [\n",
    "        torch.tensor([\n",
    "            [ 0.7933,  0.5149, -1.4123,  0.6115,  0.0221, -1.2227,  1.4272, -0.9937, -0.5665, -0.6665]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-1.6589]]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkFeedForward():\n",
    "    nn = createSampleNeuralNetwork()\n",
    "    \n",
    "    # Feste Eingabewerte für den Test\n",
    "    X = torch.tensor([\n",
    "        [ 0.1397,  0.7182],\n",
    "        [-0.5724,  2.8919]\n",
    "    ]).T\n",
    "    \n",
    "    # Eingaben mit den Gewichten verarbeiten\n",
    "    a_s = nn.predict(X)\n",
    "    \n",
    "    # Erwartete Ausgabe des letzten Layers\n",
    "    expected = torch.tensor([\n",
    "            [0.4855000078678131, 0.7422999739646912]\n",
    "    ]).T\n",
    "    \n",
    "    assert(torch.allclose(expected, a_s, rtol = 1e-04, atol = 1e-04))\n",
    "    \n",
    "checkFeedForward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBackPropagation():\n",
    "    nn = createSampleNeuralNetwork()\n",
    "    \n",
    "     # Feste Eingabewerte für den Test\n",
    "    X = torch.tensor([\n",
    "        [ 0.1397, -0.5724],\n",
    "        [ 0.7182,  2.8919]\n",
    "    ])\n",
    "\n",
    "    \n",
    "    Y = torch.tensor([\n",
    "        [0.8000],\n",
    "        [0.1000]\n",
    "    ])\n",
    "    \n",
    "    # Eingaben mit den Gewichten verarbeiten\n",
    "    z_s, a_s = nn.feedForward(X)\n",
    "    \n",
    "    # Deltas bestimmen\n",
    "    dw, db = nn.backPropagation(Y, z_s, a_s)\n",
    "    \n",
    "    dwExpected = [\n",
    "        torch.tensor([\n",
    "            [-0.0022, -0.0011, -0.0029,  0.0027, -0.0007, -0.0028, -0.0059,  0.0011, -0.0012,  0.0007],\n",
    "            [-0.0415,  0.0044,  0.0126,  0.0233, -0.0086, -0.0152, -0.0298, -0.0055, 0.0102, -0.0055]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-0.0349],\n",
    "            [-0.0525],\n",
    "            [ 0.0172],\n",
    "            [ 0.0169],\n",
    "            [ 0.0232],\n",
    "            [-0.0025],\n",
    "            [-0.0178],\n",
    "            [ 0.0193],\n",
    "            [ 0.0190],\n",
    "            [ 0.0220]\n",
    "        ])   \n",
    "    ]\n",
    "    \n",
    "    dbExpected = [\n",
    "        torch.tensor([\n",
    "            [ 0.0200, -0.0077, -0.0212, -0.0049,  0.0030, -0.0012, -0.0039,  0.0086, -0.0124,  0.0067]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-0.0222]\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    assert(torch.allclose(dw[0], dwExpected[0], rtol = 1e-04, atol = 1e-04))\n",
    "    assert(torch.allclose(dw[1], dwExpected[1], rtol = 1e-04, atol = 1e-04))\n",
    "    assert(torch.allclose(db[0], dbExpected[0], rtol = 1e-04, atol = 1e-04))    \n",
    "    assert(torch.allclose(db[1], dbExpected[1], rtol = 1e-04, atol = 1e-04))\n",
    "    \n",
    "checkBackPropagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Optimierung der Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "def sinus(activation_function, batchsize, epochsize, learning_rate, hidden_layers):\n",
    "    # TODO optimales Netzwerk ermitteln\n",
    "    sinus_nn = NeuralNetwork(layers = [1, hidden_layers, 1], activations=[activation_function, activation_function])\n",
    "\n",
    "    # sin\n",
    "    sinus_X = 2*math.pi*torch.rand(2000).reshape(-1, 1)\n",
    "    # Je nach Ausgangslayer, muss ggf. eine Transformation sattfinden\n",
    "    # Der Sigmoid hat z.B einen Wertebereich von (0,1) und könnte somit,\n",
    "    # negativen Werte des Sinus nicht darstellen.\n",
    "    sinus_y = (torch.sin(sinus_X) + 1) / 2\n",
    "\n",
    "    return sinus_nn.train(sinus_X, sinus_y, batch_size=batchsize, epochs=epochsize, lr = learning_rate)\n",
    "\n",
    "# #print(y, X)\n",
    "# plt.scatter(sinus_X.flatten(), sinus_y.flatten())\n",
    "# plt.scatter(sinus_X.flatten(), sinus_a_s.flatten())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [10 x 1], m2: [2 x 10] at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\TH/generic/THTensorMath.cpp:136",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-80b7d87cae8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnlinear_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnlinear_y\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mnlinear_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlinear_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlinear_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mnlinear_a_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlinear_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlinear_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#print(y, X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-a9796f69b0c7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x, y, batch_size, epochs, lr)\u001b[0m\n\u001b[0;32m    134\u001b[0m                     \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                     \u001b[0mz_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                     \u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdweight\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-a9796f69b0c7>\u001b[0m in \u001b[0;36mfeedForward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mz_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mactivationFunction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetActivationFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0ma_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivationFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [10 x 1], m2: [2 x 10] at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetwork(layers = [1, 10, 1], activations=['sigmoid', 'sigmoid'])\n",
    "\n",
    "nlinear_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = torch.clone(nlinear_X)\n",
    "nlinear_y[nlinear_y>=2] = 2\n",
    "nlinear_y[nlinear_y<1] = 1\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "#print(y, X)\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetwork(null)\n",
    "\n",
    "nlinear_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = torch.clone(nlinear_X)\n",
    "nlinear_y[nlinear_y>=1.5] = 2\n",
    "nlinear_y[nlinear_y<1.5] = 1\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetwork(null)\n",
    "\n",
    "nlinear_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = (10*nlinear_X**5 - 8*nlinear_X**4 - 6*nlinear_X**3 + 4*nlinear_X**2 - 2*nlinear_X + 2.3) / 2.5\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetwork(null)\n",
    "\n",
    "nlinear_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = (10*nlinear_X**5 - 8*nlinear_X**4 - 6*nlinear_X**3 + 4*nlinear_X**2 - 2*nlinear_X + 2.3) / 2.5\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-259cbd384f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.92\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mhidden_layers\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m122\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msinus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactfunction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"activation_function\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mactfunction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"batchsize\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"epochsize\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mepochsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"learning_rate\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"hidden_layers\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C://Uni/stats.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-135-4b5bfbbcd2f9>\u001b[0m in \u001b[0;36msinus\u001b[1;34m(activation_function, batchsize, epochsize, learning_rate, hidden_layers)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msinus_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msinus_X\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msinus_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msinus_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msinus_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# #print(y, X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-92c140fd7e34>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x, y, batch_size, epochs, lr)\u001b[0m\n\u001b[0;32m    135\u001b[0m                     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                     \u001b[0mz_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                     \u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdweight\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdbias\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdbias\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-92c140fd7e34>\u001b[0m in \u001b[0;36mbackPropagation\u001b[1;34m(self, y, z_s, a_s)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                 \u001b[0mdeltas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetDerivitiveActivationFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-92c140fd7e34>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-92c140fd7e34>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetDerivitiveActivationFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "actfunction = [\"sigmoid\", \"relu\", \"tanh\", \"linear\"]\n",
    "\n",
    "index = 0\n",
    "\n",
    "for i in range(len(actfunction)):\n",
    "    for batchsize in range(1, 302, 100):\n",
    "        for epochsize in range(1, 302, 100):\n",
    "            for learning_rate in numpy.arange(0.01, 0.92, 0.3):\n",
    "                for hidden_layers in range(1, 122, 40):\n",
    "                    loss = sinus(actfunction[i], batchsize, epochsize, learning_rate, hidden_layers)\n",
    "                    df = pd.DataFrame({\"activation_function\": [actfunction[i]], \"batchsize\": [batchsize], \"epochsize\": [epochsize], \"learning_rate\": [learning_rate], \"hidden_layers\": [hidden_layers],\"loss\": [loss.item()]})\n",
    "                    df.to_csv(\"C://Uni/stats.csv\", mode='a', header=False, index=False)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Implementierung Backpropagation mit Autograd und Layern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    " \n",
    "%matplotlib inline\n",
    "# Mehrschichtiges Neuronalels Netzwerk\n",
    "# Eingaben:\n",
    "#   layers: Anzahl an Neuronen pro Layer\n",
    "#   activations: verwendete Aktivierungsfunktionen pro Layer\n",
    "class NeuralNetworkTorch(nn.Module):\n",
    "    def __init__(self, layers = [1, 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        super(NeuralNetworkTorch, self).__init__()\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.torchLayers = []\n",
    "       \n",
    "        # Initialisieren der Gewichte und Bias pro Layer mit Zufallswerten\n",
    "        for i in range(len(layers)-1):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            self.torchLayers.append(layer)\n",
    "            self.add_module('layer' + str(i), layer)\n",
    "   \n",
    "    def getActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return nn.Sigmoid()\n",
    "        elif(name == 'linear'):\n",
    "            return nn.Linear()\n",
    "        elif(name == 'relu'):\n",
    "            return F.relu\n",
    "        elif(name == 'tanh'):\n",
    "            return torch.tanh\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return f.linear\n",
    "       \n",
    "    def predict(self, X):\n",
    "        return self.feedForward(X)[-1].detach()\n",
    "   \n",
    "    # Berechnung der Ausgaben\n",
    "    # Es werden alle Summen vor und nach der Aktivierungsfunktion pro Layer zurückgegeben,\n",
    "    # damit sie für die Backpropagation benutzt werden künenn\n",
    "    # Eingaben:\n",
    "    #   x: Eingabe an den ersten Layer\n",
    "    # Ausgaben:\n",
    "    #   a_s: Ausgaben pro Layer\n",
    "    def feedForward(self, x):\n",
    "        a = x.clone()\n",
    "        a_s = [a]\n",
    " \n",
    "        for i in range(len(self.torchLayers)):\n",
    "            a = self.torchLayers[i](a)\n",
    "        return a\n",
    " \n",
    "    # Aktualisieren der Gewichte und Bias anhand der Ausgabe in mehreren Batches und Epochen\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "        for e in range(epochs):\n",
    "            i=0\n",
    "            while(i<len(y)):\n",
    "                x_batch = Variable(x[i:i+batch_size])\n",
    "                y_batch = Variable(y[i:i+batch_size])\n",
    "                i = i+batch_size\n",
    " \n",
    "                a_s = self.feedForward(x_batch)\n",
    "                criterion = torch.nn.MSELoss(reduction='sum')\n",
    " \n",
    "                loss = criterion(a_s[-1], y)\n",
    "                loss.backward()\n",
    " \n",
    "                #TODO Training mit Pytorch\n",
    "                print(f\"loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetworkTorch(null)\n",
    "\n",
    "nlinear_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = (10*nlinear_X**5 - 8*nlinear_X**4 - 6*nlinear_X**3 + 4*nlinear_X**2 - 2*nlinear_X + 2.3) / 2.5\n",
    "\n",
    "print(nlinear_X.shape)\n",
    "print(nlinear_y.shape)\n",
    "print(nlinear_nn)\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "#print(y, X)\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetworkTorch(null)\n",
    "\n",
    "nlinear_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = torch.clone(nlinear_X)\n",
    "nlinear_y[nlinear_y>=1.5] = 2\n",
    "nlinear_y[nlinear_y<1.5] = 1\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weitere optimale Netzwerke mit der PytorchImplementierung für die anderen Probleme ermitteln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Alternative Optimierungsalgorithmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weitere Optimierungsalgorithmen von Pytorch für die Probleme ausprobieren und bewerten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit49fff36c31ad458c9f75859cc9990ad8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}