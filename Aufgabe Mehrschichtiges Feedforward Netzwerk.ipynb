{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mehrschichtiges feedforward-Netz\n",
    "Nachfolgend sollen die Methoden `feedForward`, `backPropagation` vervollständigt werden.\n",
    "Um die Implementierungen zu prüfen, können die Methoden `checkFeedForward` und `checkBackPropagation` verwendet werden.\n",
    "Anschließend werden verschiedene Architekturen (Layer, Anzahl Neuronen, Aktivierungsfunktionen, Batch-Sizes, Learningrates) manuell ausgetestet werden, um für die Beispiel-Funktionen optimale Ergebnisse zu erzielen. Danach folgt eine Implementierung mit `torch.nn`, `torch.autograd` und `torch.optim` und ein weiterer Test der Architekturen. Abschließend werden alternatvie Optimierungsalgorithmen ausgetestet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Implementierung Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "%matplotlib inline\n",
    "# Mehrschichtiges Neuronalels Netzwerk\n",
    "# Eingaben:\n",
    "#   layers: Anzahl an Neuronen pro Layer\n",
    "#   activations: verwendete Aktivierungsfunktionen pro Layer\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.weights = [] # Shape in Layer-i [Neuronen in i, Neuronen in i+1]\n",
    "        self.biases = []  # Shape in Layer-i [1, Neuronen in i+1]\n",
    "        \n",
    "        # Initialisieren der Gewichte und Bias pro Layer mit Zufallswerten\n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(self.initializeWeight(layers[i], layers[i+1], self.activations[i]))\n",
    "            self.biases.append(self.initializeWeight(1, layers[i+1], self.activations[i]))\n",
    "    \n",
    "    def initializeWeight(self, outputDim, inputDim, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return torch.randn(outputDim, inputDim)\n",
    "        elif(name == 'linear'):\n",
    "            tensor = torch.empty(outputDim, inputDim)\n",
    "            torch.nn.init.kaiming_uniform_(tensor, a=math.sqrt(5))\n",
    "            return tensor\n",
    "        elif(name == 'relu'):\n",
    "            return torch.randn(outputDim, inputDim)*math.sqrt(2/inputDim)\n",
    "        elif(name == 'tanh'):\n",
    "            return torch.randn(outputDim, inputDim)*math.sqrt(1/inputDim)\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            tensor = torch.empty(outputDim, inputDim)\n",
    "            torch.nn.init.kaiming_uniform_(tensor, a=math.sqrt(5))\n",
    "            return tensor\n",
    "    \n",
    "    def getActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : torch.exp(x)/(1+torch.exp(x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = torch.clone(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        elif(name == 'tanh'):\n",
    "            return lambda x : torch.tanh(x)\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return lambda x: x\n",
    "\n",
    "    def getDerivitiveActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : torch.exp(x)/(1+torch.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = torch.clone(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        elif(name == 'tanh'):\n",
    "            return lambda x : 1 - torch.tanh(x)**2\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return lambda x: 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        (_, a_s) = self.feedForward(X)\n",
    "        return a_s[-1]\n",
    "    \n",
    "    # Berechnung der Ausgaben\n",
    "    # Es werden alle Summen vor und nach der Aktivierungsfunktion pro Layer zurückgegeben,\n",
    "    # damit sie für die Backpropagation benutzt werden künenn\n",
    "    # Eingaben:\n",
    "    #   x: Eingabe an den ersten Layer [batch_size, 1]\n",
    "    # Ausgaben:\n",
    "    #   z_s: Summen (vor Aktivierungsfunktion) pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   a_s: Ausgaben pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    def feedForward(self, x):\n",
    "        a = x.clone()\n",
    "        z_s = []\n",
    "        a_s = [a] \n",
    "                      \n",
    "        for l in range(len(self.weights)):\n",
    "            z_s.append(a.mm(self.weights[l]) + self.biases[l])\n",
    "            activationFunction = self.getActivationFunction(self.activations[l])\n",
    "            a_s.append(activationFunction(z_s[-1]))\n",
    "            a = a_s[-1]\n",
    "\n",
    "        return (z_s, a_s)\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Zurückverteilen der Deltas an die Gewichte und Biases\n",
    "    # Eingaben:\n",
    "    #   z_s: Summen (vor Aktivierungsfunktion) pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   a_s: Ausgaben pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    # Ausgaben:\n",
    "    #   dw: Ableitungen bezüglich der Gewichte pro Layer, [batch_size, Neuronen in Layer i] \n",
    "    #   db: Ableitungen bezüglich der Biases pro Layer, [batch_size, 1] \n",
    "    def backPropagation(self, y, z_s, a_s):\n",
    "            dw = []  # dC/dW\n",
    "            db = []  # dC/dB\n",
    "            deltas = [None] * len(self.weights)  # delta = dC/dZ Fehler pro Layer\n",
    "            # Füllen des Fehlers im letzten Layer anhand der erwarteten Ausgaben des Netzwerks\n",
    "            deltas[-1] = (y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1])(z_s[-1]))\n",
    "                       \n",
    "            # Ausführen der Backpropagation\n",
    "            # Füllen des Fehlers im Layer anhand der bisherigen Deltas\n",
    "            \n",
    "            for k in reversed(range(len(deltas)-1)):\n",
    "                deltas[k] = deltas[k+1].mm(self.weights[k+1].T)*(self.getDerivitiveActivationFunction(self.activations[k])(z_s[k]))\n",
    "                \n",
    "            batch_size = y.shape[0]\n",
    "            # Bestimmen der Gewichtsänderung\n",
    "            db = [torch.ones(1, batch_size).mm(d/float(batch_size)) for d in deltas]\n",
    "            dw = [a_s[i].T.mm(d)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "\n",
    "            # Rückgabe der Ableitungen bezüglich der Gewichte und Biases\n",
    "            return dw, db\n",
    "\n",
    "    # Aktualisieren der Gewichte und Bias anhand der Ausgabe in mehreren Batches und Epochen\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "            for e in range(epochs): \n",
    "                i=0\n",
    "                while(i<len(y)):\n",
    "                    x_batch = x[i:i+batch_size]\n",
    "                    y_batch = y[i:i+batch_size]\n",
    "                    i = i+batch_size\n",
    "                    z_s, a_s = self.feedForward(x_batch)\n",
    "                    dw, db = self.backPropagation(y_batch, z_s, a_s)\n",
    "                    self.weights = [w+lr*dweight for w,dweight in zip(self.weights, dw)]\n",
    "                    self.biases = [w+lr*dbias for w,dbias in zip(self.biases, db)]\n",
    "                    loss = torch.norm(a_s[-1]-y_batch)\n",
    "            return loss\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSampleNeuralNetwork():\n",
    "    nn = NeuralNetwork(layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid'])\n",
    "    \n",
    "    # Feste Gewichte und Bias setzen, damit das Ergebnis immer gleich ist.\n",
    "    nn.weights = [\n",
    "        torch.tensor([\n",
    "            [ 0.1397, -0.5724,  0.2194, -3.0984,  0.6770, -0.3220,  1.4311,  0.0931, -0.7618,  0.6987],\n",
    "            [ 0.7182,  2.8919, -1.9848, -0.2261, -1.1376, -0.0176, -0.2824, -1.6781, -1.1798, -1.5255]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [ 3.0417],\n",
    "            [-1.1143],\n",
    "            [-2.2069],\n",
    "            [-1.1303],\n",
    "            [ 0.5879],\n",
    "            [ 0.5013],\n",
    "            [ 1.1043],\n",
    "            [ 0.8942],\n",
    "            [-1.3571],\n",
    "            [ 0.7487]\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    nn.biases = [\n",
    "        torch.tensor([\n",
    "            [ 0.7933,  0.5149, -1.4123,  0.6115,  0.0221, -1.2227,  1.4272, -0.9937, -0.5665, -0.6665]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-1.6589]]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkFeedForward():\n",
    "    nn = createSampleNeuralNetwork()\n",
    "    \n",
    "    # Feste Eingabewerte für den Test\n",
    "    X = torch.tensor([\n",
    "        [ 0.1397,  0.7182],\n",
    "        [-0.5724,  2.8919]\n",
    "    ]).T\n",
    "    \n",
    "    # Eingaben mit den Gewichten verarbeiten\n",
    "    a_s = nn.predict(X)\n",
    "    \n",
    "    # Erwartete Ausgabe des letzten Layers\n",
    "    expected = torch.tensor([\n",
    "            [0.4855000078678131, 0.7422999739646912]\n",
    "    ]).T\n",
    "    \n",
    "    assert(torch.allclose(expected, a_s, rtol = 1e-04, atol = 1e-04))\n",
    "    \n",
    "checkFeedForward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBackPropagation():\n",
    "    nn = createSampleNeuralNetwork()\n",
    "    \n",
    "     # Feste Eingabewerte für den Test\n",
    "    X = torch.tensor([\n",
    "        [ 0.1397, -0.5724],\n",
    "        [ 0.7182,  2.8919]\n",
    "    ])\n",
    "\n",
    "    \n",
    "    Y = torch.tensor([\n",
    "        [0.8000],\n",
    "        [0.1000]\n",
    "    ])\n",
    "    \n",
    "    # Eingaben mit den Gewichten verarbeiten\n",
    "    z_s, a_s = nn.feedForward(X)\n",
    "    \n",
    "    # Deltas bestimmen\n",
    "    dw, db = nn.backPropagation(Y, z_s, a_s)\n",
    "    \n",
    "    dwExpected = [\n",
    "        torch.tensor([\n",
    "            [-0.0022, -0.0011, -0.0029,  0.0027, -0.0007, -0.0028, -0.0059,  0.0011, -0.0012,  0.0007],\n",
    "            [-0.0415,  0.0044,  0.0126,  0.0233, -0.0086, -0.0152, -0.0298, -0.0055, 0.0102, -0.0055]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-0.0349],\n",
    "            [-0.0525],\n",
    "            [ 0.0172],\n",
    "            [ 0.0169],\n",
    "            [ 0.0232],\n",
    "            [-0.0025],\n",
    "            [-0.0178],\n",
    "            [ 0.0193],\n",
    "            [ 0.0190],\n",
    "            [ 0.0220]\n",
    "        ])   \n",
    "    ]\n",
    "    \n",
    "    dbExpected = [\n",
    "        torch.tensor([\n",
    "            [ 0.0200, -0.0077, -0.0212, -0.0049,  0.0030, -0.0012, -0.0039,  0.0086, -0.0124,  0.0067]\n",
    "        ]),\n",
    "        torch.tensor([\n",
    "            [-0.0222]\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    assert(torch.allclose(dw[0], dwExpected[0], rtol = 1e-04, atol = 1e-04))\n",
    "    assert(torch.allclose(dw[1], dwExpected[1], rtol = 1e-04, atol = 1e-04))\n",
    "    assert(torch.allclose(db[0], dbExpected[0], rtol = 1e-04, atol = 1e-04))    \n",
    "    assert(torch.allclose(db[1], dbExpected[1], rtol = 1e-04, atol = 1e-04))\n",
    "    \n",
    "checkBackPropagation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Optimierung der Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "def sinus(activation_function, batchsize, epochsize, learning_rate, hidden_layers):\n",
    "    # TODO optimales Netzwerk ermitteln\n",
    "    sinus_nn = NeuralNetwork(layers = [1, hidden_layers, 1], activations=[activation_function, activation_function])\n",
    "\n",
    "    # sin\n",
    "    sinus_X = 2*math.pi*torch.rand(2000).reshape(-1, 1)\n",
    "    # Je nach Ausgangslayer, muss ggf. eine Transformation sattfinden\n",
    "    # Der Sigmoid hat z.B einen Wertebereich von (0,1) und könnte somit,\n",
    "    # negativen Werte des Sinus nicht darstellen.\n",
    "    sinus_y = (torch.sin(sinus_X) + 1) / 2\n",
    "\n",
    "    return sinus_nn.train(sinus_X, sinus_y, batch_size=batchsize, epochs=epochsize, lr = learning_rate)\n",
    "\n",
    "# #print(y, X)\n",
    "# plt.scatter(sinus_X.flatten(), sinus_y.flatten())\n",
    "# plt.scatter(sinus_X.flatten(), sinus_a_s.flatten())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def linear1(activation_function, batchsize, epochsize, learning_rate, hidden_layers):\n",
    "    nlinear1_nn = NeuralNetwork(layers = [1, hidden_layers, 1], activations=[activation_function, activation_function])\n",
    "\n",
    "    nlinear1_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "    nlinear1_y = torch.clone(nlinear_X)\n",
    "    nlinear1_y[nlinear1_y>=2] = 2\n",
    "    nlinear1_y[nlinear1_y<1] = 1\n",
    "\n",
    "    return nlinear1_nn.train(nlinear1_X, nlinear1_y, batch_size=batchsize, epochs=epochsize, lr = learning_rate)\n",
    "\n",
    "\n",
    "#print(y, X)\n",
    "# plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "# plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear2(activation_function, batchsize, epochsize, learning_rate, hidden_layers):\n",
    "    nlinear2_nn = NeuralNetwork(layers = [1, hidden_layers, 1], activations=[activation_function, activation_function])\n",
    "\n",
    "    nlinear2_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "    nlinear2_y = torch.clone(nlinear2_X)\n",
    "    nlinear2_y[nlinear2_y>=1.5] = 2\n",
    "    nlinear2_y[nlinear2_y<1.5] = 1\n",
    "\n",
    "    return nlinear2_nn.train(nlinear2_X, nlinear2_y, batch_size=batchsize, epochs=epochsize, lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear3(activation_function, batchsize, epochsize, learning_rate, hidden_layers):\n",
    "    nlinear3_nn = NeuralNetwork(layers = [1, hidden_layers, 1], activations=[activation_function, activation_function])\n",
    "\n",
    "    nlinear3_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "    nlinear3_y = (10*nlinear3_X**5 - 8*nlinear3_X**4 - 6*nlinear3_X**3 + 4*nlinear3_X**2 - 2*nlinear3_X + 2.3) / 2.5\n",
    "\n",
    "    return nlinear3_nn.train(nlinear3_X, nlinear3_y, batch_size=batchsize, epochs=epochsize, lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear4(activation_function, batchsize, epochsize, learning_rate, hidden_layers):\n",
    "    nlinear4_nn = NeuralNetwork(layers = [1, hidden_layers, 1], activations=[activation_function, activation_function])\n",
    "\n",
    "    nlinear4_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "    nlinear4_y = (10*nlinear4_X**5 - 8*nlinear4_X**4 - 6*nlinear4_X**3 + 4*nlinear4_X**2 - 2*nlinear4_X + 2.3) / 2.5\n",
    "\n",
    "    return nlinear4_nn.train(nlinear4_X, nlinear4_y,  batch_size=batchsize, epochs=epochsize, lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "actfunction = [\"sigmoid\", \"relu\", \"tanh\", \"linear\"]\n",
    "\n",
    "for i in range(len(actfunction)):\n",
    "    for batchsize in range(1, 302, 100):\n",
    "        for epochsize in range(1, 302, 100):\n",
    "            for learning_rate in numpy.arange(0.01, 0.92, 0.3):\n",
    "                for hidden_layers in range(1, 122, 40):\n",
    "                    loss = linear1(actfunction[i], batchsize, epochsize, learning_rate, hidden_layers)\n",
    "                    loss2 = linear2(actfunction[i], batchsize, epochsize, learning_rate, hidden_layers)\n",
    "                    loss3 = linear3(actfunction[i], batchsize, epochsize, learning_rate, hidden_layers)\n",
    "                    loss4 = linear4(actfunction[i], batchsize, epochsize, learning_rate, hidden_layers)\n",
    "                    df = pd.DataFrame({\"activation_function\": [actfunction[i]], \"batchsize\": [batchsize], \"epochsize\": [epochsize], \"learning_rate\": [learning_rate], \"hidden_layers\": [hidden_layers],\"loss\": [loss.item()]})\n",
    "                    df2 = pd.DataFrame({\"activation_function\": [actfunction[i]], \"batchsize\": [batchsize], \"epochsize\": [epochsize], \"learning_rate\": [learning_rate], \"hidden_layers\": [hidden_layers],\"loss\": [loss2.item()]})\n",
    "                    df3 = pd.DataFrame({\"activation_function\": [actfunction[i]], \"batchsize\": [batchsize], \"epochsize\": [epochsize], \"learning_rate\": [learning_rate], \"hidden_layers\": [hidden_layers],\"loss\": [loss3.item()]})\n",
    "                    df4 = pd.DataFrame({\"activation_function\": [actfunction[i]], \"batchsize\": [batchsize], \"epochsize\": [epochsize], \"learning_rate\": [learning_rate], \"hidden_layers\": [hidden_layers],\"loss\": [loss4.item()]})\n",
    "                    df.to_csv(\"C://Uni/stats_linear1.csv\", mode='a', header=False, index=False)\n",
    "                    df2.to_csv(\"C://Uni/stats_linear2.csv\", mode='a', header=False, index=False)\n",
    "                    df3.to_csv(\"C://Uni/stats_linear3.csv\", mode='a', header=False, index=False)\n",
    "                    df4.to_csv(\"C://Uni/stats_linear4.csv\", mode='a', header=False, index=False)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-a750e5e38b38>, line 13)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-47-a750e5e38b38>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    df =\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\"C://Uni//stats_sinus.csv\", \"C://Uni//stats_linear1.csv\", \"C://Uni//stats_linear2.csv\", \"C://Uni//stats_linear3.csv\", \"C://Uni//stats_linear4.csv\", ]\n",
    "frames = []\n",
    "\n",
    "for file in range(len(files)):\n",
    "    df = pd.read_csv(files[file])\n",
    "    headers = [\"activation_function\", 'batchsize', 'epochsize', 'learning_rate', 'hidden_layers', \"loss\"]\n",
    "    df.columns = headers \n",
    "    frames.append((files[file],df))\n",
    "\n",
    "for frame in frames:\n",
    "    min_lines = frame[1][frame[1].loss != 0].nsmallest(3, [\"loss\"])\n",
    "    print(f\"The least loss in file {os.path.basename(frame[0])} was achieved with:\")\n",
    "    print(min_lines.to_string(index=False))\n",
    "    print(\"----------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Implementierung Backpropagation mit Autograd und Layern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "# Mehrschichtiges Neuronalels Netzwerk\n",
    "# Eingaben:\n",
    "#   layers: Anzahl an Neuronen pro Layer\n",
    "#   activations: verwendete Aktivierungsfunktionen pro Layer\n",
    "class NeuralNetworkTorch(nn.Module):\n",
    "    def __init__(self, layers = [1, 100, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        super(NeuralNetworkTorch, self).__init__()\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.torchLayers = []\n",
    "        \n",
    "        # Initialisieren der Gewichte und Bias pro Layer mit Zufallswerten\n",
    "        for i in range(len(layers)-1):\n",
    "            layer = nn.Linear(layers[i], layers[i+1])\n",
    "            self.torchLayers.append(layer)\n",
    "            self.add_module('layer' + str(i), layer)\n",
    "    \n",
    "    def getActivationFunction(self, name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return nn.Sigmoid()\n",
    "        elif(name == 'linear'):\n",
    "            return nn.Linear()\n",
    "        elif(name == 'relu'):\n",
    "            return F.relu\n",
    "        elif(name == 'tanh'):\n",
    "            return torch.tanh\n",
    "        else:\n",
    "            print('Unbekannte Aktivierungsfunktion. linear wird verwendet')\n",
    "            return f.linear\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.feedForward(X)[-1].detach()\n",
    "    \n",
    "    # Berechnung der Ausgaben\n",
    "    # Es werden alle Summen vor und nach der Aktivierungsfunktion pro Layer zurückgegeben,\n",
    "    # damit sie für die Backpropagation benutzt werden künenn\n",
    "    # Eingaben:\n",
    "    #   x: Eingabe an den ersten Layer\n",
    "    # Ausgaben:\n",
    "    #   a_s: Ausgaben pro Layer\n",
    "    def feedForward(self, x):\n",
    "        a = x.clone()\n",
    "        a_s = [a]\n",
    "\n",
    "        for i in range(len(self.torchLayers)):\n",
    "            a = self.torchLayers[i](a)\n",
    "        return a\n",
    "\n",
    "    # Aktualisieren der Gewichte und Bias anhand der Ausgabe in mehreren Batches und Epochen\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "        criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for e in range(epochs): \n",
    "            i=0\n",
    "            while(i<len(y)):\n",
    "                x_batch = Variable(x[i:i+batch_size])\n",
    "                y_batch = Variable(y[i:i+batch_size])\n",
    "                i = i+batch_size\n",
    "\n",
    "                a_s = self.feedForward(x_batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(a_s[-1], y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #TODO Training mit Pytorch\n",
    "                print(f\"loss = {torch.norm(a_s[-1]-y_batch)}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetworkTorch(null)\n",
    "\n",
    "nlinear_X = 1.14*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = (10*nlinear_X**5 - 8*nlinear_X**4 - 6*nlinear_X**3 + 4*nlinear_X**2 - 2*nlinear_X + 2.3) / 2.5\n",
    "\n",
    "print(nlinear_X.shape)\n",
    "print(nlinear_y.shape)\n",
    "print(nlinear_nn)\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=null, batch_size=null, lr = null)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "#print(y, X)\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# TODO optimales Netzwerk ermitteln\n",
    "nlinear_nn = NeuralNetworkTorch(layers = [1, 10, 1])\n",
    "\n",
    "nlinear_X = 3*torch.rand(2000).reshape(-1, 1)\n",
    "nlinear_y = torch.clone(nlinear_X)\n",
    "nlinear_y[nlinear_y>=1.5] = 2\n",
    "nlinear_y[nlinear_y<1.5] = 1\n",
    "\n",
    "nlinear_nn.train(nlinear_X, nlinear_y, epochs=10, batch_size=1, lr = 0.0001)\n",
    "nlinear_a_s = nlinear_nn.predict(nlinear_X)\n",
    "\n",
    "print(nlinear_a_s)\n",
    "\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_y.flatten())\n",
    "plt.scatter(nlinear_X.flatten(), nlinear_a_s.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weitere optimale Netzwerke mit der PytorchImplementierung für die anderen Probleme ermitteln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Alternative Optimierungsalgorithmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO weitere Optimierungsalgorithmen von Pytorch für die Probleme ausprobieren und bewerten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit49fff36c31ad458c9f75859cc9990ad8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}